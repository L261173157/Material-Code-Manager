# -*- coding: utf-8 -*-
"""
ææ–™å‹å·è§„èŒƒåŒ–æ¨¡å‹è®­ç»ƒè„šæœ¬
=========================

åŸºäº ByT5 + LoRA çš„åºåˆ—åˆ°åºåˆ—æ–‡æœ¬è§„èŒƒåŒ–æ¨¡å‹è®­ç»ƒç¨‹åºã€‚

ç‰¹æ€§ï¼š
- ğŸš€ å­—ç¬¦çº§åºåˆ—å»ºæ¨¡ï¼Œæ— éœ€åˆ†è¯
- ğŸ’¡ LoRA ä½ç§©é€‚åº”ï¼Œå¤§å¹…é™ä½è®­ç»ƒå‚æ•°
- ğŸ‡¨ğŸ‡³ æ”¯æŒå›½å†… HuggingFace é•œåƒ
- ğŸ”„ è‡ªåŠ¨æ¨¡å‹ä¿å­˜å’Œæ–­ç‚¹æ¢å¤
- ğŸ“Š è¯¦ç»†è®­ç»ƒæ—¥å¿—å’Œè¿›åº¦ç›‘æ§

é€‚ç”¨ç¯å¢ƒï¼šWindows + Conda + å›½å†…ç½‘ç»œç¯å¢ƒ

ä½œè€…ï¼šMaterial-Code-Manager é¡¹ç›®ç»„
"""ing: utf-8 -*-
"""
train_norm.py  â€”â€” ç®€æ´ç‰ˆï¼ˆTransformers 4.56.0+ï¼‰
ç‰¹æ€§ï¼š
- å…ˆè¯» config å†™å…¥ HF_ENDPOINT/HF_HOMEï¼ˆé•œåƒ/ç¼“å­˜ï¼‰ï¼Œå†å¯¼å…¥ transformers
- --offline æ”¯æŒå®Œå…¨ç¦»çº¿(local_files_only + TRANSFORMERS_OFFLINE=1)
- å¿ƒè·³æ—¥å¿—ï¼šæ¯ 10 ç§’æ‰“å°â€œç¨‹åºä»åœ¨è¿è¡Œâ€
- æ¸…æ™°é˜¶æ®µæ ‡å¿—ï¼šåŠ è½½æ•°æ® / åŠ è½½æ¨¡å‹ / è®­ç»ƒ / è¯„ä¼° / ä¿å­˜
"""

import os
import sys
import time
import yaml
import argparse
import threading
import numpy as np

# ============ å·¥å…·å‡½æ•° ============

RUNNING = True  # å…¨å±€è¿è¡Œæ ‡å¿—

def log(msg: str):
    """è¾“å‡ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—ä¿¡æ¯"""
    ts = time.strftime("%H:%M:%S")
    print(f"[{ts}] {msg}", flush=True)

def heartbeat(msg="ğŸ’¡ è®­ç»ƒè¿›è¡Œä¸­ï¼Œè¯·ç¨å€™..."):
    """åå°å¿ƒè·³çº¿ç¨‹ï¼Œæ¯10ç§’è¾“å‡ºä¸€æ¬¡çŠ¶æ€"""
    while RUNNING:
        time.sleep(10)
        print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    p = argparse.ArgumentParser(
        description="ææ–™å‹å·è§„èŒƒåŒ–æ¨¡å‹è®­ç»ƒç¨‹åº",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python src/train_norm.py --config configs/config.yaml --csv data/train_pairs.csv --epochs 10
  python src/train_norm.py --config configs/config.yaml --offline  # ç¦»çº¿æ¨¡å¼
        """
    )
    p.add_argument("--config", type=str, default="configs/config.yaml",
                   help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: configs/config.yaml)")
    p.add_argument("--csv", type=str, default=None,
                   help="è®­ç»ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®")
    p.add_argument("--epochs", type=int, default=None,
                   help="è®­ç»ƒè½®æ•°ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®")
    p.add_argument("--offline", action="store_true",
                   help="å®Œå…¨ç¦»çº¿è¿è¡Œæ¨¡å¼ï¼ˆéœ€è¦é¢„å…ˆä¸‹è½½æ¨¡å‹ï¼‰")
    return p.parse_args()

def load_config_and_set_env(path):
    """
    åŠ è½½é…ç½®æ–‡ä»¶å¹¶è®¾ç½®ç¯å¢ƒå˜é‡
    æ³¨æ„ï¼šå¿…é¡»åœ¨å¯¼å…¥transformersä¹‹å‰è°ƒç”¨ï¼Œä»¥æ­£ç¡®è®¾ç½®HFé•œåƒ
    """
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆHuggingFaceé•œåƒé…ç½®ç­‰ï¼‰
    for k, v in (cfg.get("env") or {}).items():
        os.environ[str(k)] = str(v)
        log(f"ç¯å¢ƒå˜é‡è®¾ç½®: {k}={v}")
    
    return cfg

# ============ ä¸»ç¨‹åºå…¥å£ ============

# 1. è§£æå‚æ•°å¹¶åŠ è½½é…ç½®ï¼ˆåœ¨å¯¼å…¥transformersä¹‹å‰å®Œæˆï¼‰
args = parse_args()
log("ğŸš€ å¼€å§‹åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ...")
cfg = load_config_and_set_env(args.config)

# 2. å‚æ•°è¦†ç›–
if args.csv:
    cfg["data"]["csv_path"] = args.csv
    log(f"ğŸ“Š è®­ç»ƒæ•°æ®è·¯å¾„: {args.csv}")
if args.epochs:
    cfg["train"]["num_train_epochs"] = args.epochs
if args.offline:
    os.environ["TRANSFORMERS_OFFLINE"] = "1"

log(f"HF_ENDPOINT = {os.environ.get('HF_ENDPOINT')}")
log(f"HF_HOME     = {os.environ.get('HF_HOME')}")
log(f"TRANSFORMERS_OFFLINE = {os.environ.get('TRANSFORMERS_OFFLINE')}")

# ç°åœ¨å†å¯¼å…¥ä¾èµ–ï¼ˆç¡®ä¿ env å·²ç”Ÿæ•ˆï¼‰
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments, Seq2SeqTrainer
)
from peft import LoraConfig, get_peft_model
from rapidfuzz.distance import Levenshtein

# ============ æŒ‡æ ‡ ============
def compute_metrics_builder(tokenizer, prefix):
    def compute_metrics(eval_pred):
        preds, labels = eval_pred
        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)
        label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)
        pred_texts = [t.replace(prefix, "").strip() for t in pred_texts]
        exact = sum(int(p == y) for p, y in zip(pred_texts, label_texts))
        cer_sum = sum(Levenshtein.distance(p, y) / max(1, len(y)) for p, y in zip(pred_texts, label_texts))
        n = max(1, len(pred_texts))
        return {"exact_match": exact / n, "cer": cer_sum / n}
    return compute_metrics

# ============ ä¸»æµç¨‹ ============
def main():
    global RUNNING
    print("\n==== ğŸš€ ç¨‹åºå¯åŠ¨ ====\n", flush=True)

    # å¿ƒè·³
    t = threading.Thread(target=heartbeat, args=("ğŸ’¡ æ­£åœ¨è¿è¡Œï¼Œè¯·ç¨å€™...",), daemon=True)
    t.start()

    # 1) æ•°æ®
    csv_path = cfg["data"]["csv_path"]
    assert os.path.exists(csv_path), f"CSV not found: {csv_path}"
    log("==== ğŸ“Š åŠ è½½æ•°æ®é›† ====")
    raw = load_dataset("csv", data_files=csv_path, split="train")
    raw = raw.train_test_split(test_size=cfg["data"]["test_size"], seed=cfg["train"]["seed"])
    train_valid = raw["train"].train_test_split(test_size=cfg["data"]["valid_size_from_train"], seed=cfg["train"]["seed"])
    train_ds, valid_ds, test_ds = train_valid["train"], train_valid["test"], raw["test"]
    log(f"æ•°æ®åˆ‡åˆ†ï¼štrain={len(train_ds)}  valid={len(valid_ds)}  test={len(test_ds)}")

    # 2) æ¨¡å‹
    log("==== ğŸ¤– åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ====")
    local_only = bool(os.environ.get("TRANSFORMERS_OFFLINE") == "1")
    name_or_path = cfg["model"]["name"]
    try:
        tok = AutoTokenizer.from_pretrained(name_or_path, local_files_only=local_only)
        model = AutoModelForSeq2SeqLM.from_pretrained(name_or_path, local_files_only=local_only)
    except OSError as e:
        log(f"âš ï¸ åœ¨çº¿/ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°è¯•æœ¬åœ°åŠ è½½ï¼š{e}")
        tok = AutoTokenizer.from_pretrained(name_or_path, local_files_only=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(name_or_path, local_files_only=True)
        log("âœ… æœ¬åœ°åŠ è½½æˆåŠŸ")

    if cfg["model"]["lora"]["enable"]:
        log("==== ğŸ§© æŒ‚è½½ LoRA é€‚é…å±‚ ====")
        lcfg = cfg["model"]["lora"]
        model = get_peft_model(model, LoraConfig(
            r=lcfg["r"], lora_alpha=lcfg["alpha"],
            target_modules=lcfg["target_modules"], lora_dropout=lcfg["dropout"],
            bias="none", task_type="SEQ_2_SEQ_LM"
        ))
        log("LoRA æŒ‚è½½å®Œæˆ")

    # 3) é¢„å¤„ç†æ˜ å°„
    log("==== ğŸ§ª å‡†å¤‡æ•°æ®æ˜ å°„ ====")
    MAX_LEN = cfg["train"]["max_length"]; PREFIX = cfg["train"]["prefix"]
    def preprocess(ex):
        inp = (ex.get("dirty") or "").strip()
        tgt = (ex.get("clean") or "").strip()
        enc = tok(PREFIX + inp, max_length=MAX_LEN, truncation=True)
        with tok.as_target_tokenizer():
            labels = tok(tgt, max_length=MAX_LEN, truncation=True)
        enc["labels"] = labels["input_ids"]
        return enc
    train_ds = train_ds.map(preprocess, remove_columns=train_ds.column_names)
    valid_ds = valid_ds.map(preprocess, remove_columns=valid_ds.column_names)
    test_ds_prep = test_ds.map(preprocess, remove_columns=test_ds.column_names)
    log("æ•°æ®æ˜ å°„å®Œæˆ")

    # 4) è®­ç»ƒ
    log("==== ğŸ¯ å¼€å§‹è®­ç»ƒ ====")
    ta = Seq2SeqTrainingArguments(
        output_dir=cfg["train"]["save_dir"],
        per_device_train_batch_size=cfg["train"]["batch_size"],
        per_device_eval_batch_size=cfg["train"]["eval_batch_size"],
        gradient_accumulation_steps=cfg["train"]["gradient_accumulation_steps"],
        learning_rate=float(cfg["train"]["learning_rate"]),
        num_train_epochs=cfg["train"]["num_train_epochs"],
        weight_decay=float(cfg["train"]["weight_decay"]),
        logging_steps=cfg["train"]["logging_steps"],
        eval_strategy=cfg["train"]["eval_strategy"],  # ä¿®æ”¹è¿™é‡Œï¼ševaluation_strategy -> eval_strategy
        save_strategy=cfg["train"]["save_strategy"],
        load_best_model_at_end=True,
        metric_for_best_model=cfg["train"]["metric_for_best"],
        greater_is_better=cfg["train"]["greater_is_better"],
        predict_with_generate=True,
        fp16=cfg["train"]["fp16"],
        label_smoothing_factor=float(cfg["train"].get("label_smoothing", 0.0)),
        seed=cfg["train"]["seed"],
    )
    trainer = Seq2SeqTrainer(
        model=model,
        args=ta,
        train_dataset=train_ds,
        eval_dataset=valid_ds,
        data_collator=DataCollatorForSeq2Seq(tok, model=model),
        tokenizer=tok,
        compute_metrics=compute_metrics_builder(tok, PREFIX),
    )

    trainer.train()
    log("âœ… è®­ç»ƒå®Œæˆ")

    # 5) è¯„ä¼°
    log("==== ğŸ“ˆ æµ‹è¯•é›†è¯„ä¼° ====")
    test_metrics = trainer.evaluate(test_ds_prep)
    print("TEST:", test_metrics, flush=True)

    # 6) ä¿å­˜
    log("==== ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ ====")
    best_dir = os.path.join(cfg["train"]["save_dir"], "best")
    os.makedirs(best_dir, exist_ok=True)
    trainer.save_model(best_dir)
    tok.save_pretrained(best_dir)
    log(f"æ¨¡å‹å·²ä¿å­˜ï¼š{best_dir}")

    RUNNING = False
    print("\n==== âœ… ç¨‹åºç»“æŸ ====\n", flush=True)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        RUNNING = False
        log("ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰")
        sys.exit(130)
    except Exception as ex:
        RUNNING = False
        log(f"âŒ ç¨‹åºå¼‚å¸¸ï¼š{ex}")
        raise
