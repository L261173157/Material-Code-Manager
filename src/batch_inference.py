# -*- coding: utf-8 -*-
"""
ææ–™å‹å·è§„èŒƒåŒ–æ‰¹é‡æ¨ç†è„šæœ¬
========================

åŸºäºè®­ç»ƒå¥½çš„ ByT5 + LoRA æ¨¡å‹è¿›è¡Œæ‰¹é‡æ–‡æœ¬è§„èŒƒåŒ–ï¼Œå¹¶è¾“å‡ºç½®ä¿¡åº¦è¯„åˆ†ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ“Š æ‰¹é‡CSVæ–‡ä»¶å¤„ç†ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®
- ğŸ¯ è¾“å‡ºæ¨ç†ç»“æœå’Œç½®ä¿¡åº¦è¯„åˆ†
- ğŸ“ˆ å®æ—¶è¿›åº¦æ˜¾ç¤ºå’Œé”™è¯¯å¤„ç†
- ğŸ‡¨ğŸ‡³ å†…ç½®å›½å†…é•œåƒæ”¯æŒï¼Œé€‚é…ä¸­æ–‡ç¯å¢ƒ
- ğŸ’¾ è‡ªåŠ¨ç»“æœä¿å­˜å’Œæ¢å¤æœºåˆ¶

è¾“å…¥æ ¼å¼ï¼šCSV æ–‡ä»¶ï¼ŒåŒ…å« 'dirty' åˆ—
è¾“å‡ºæ ¼å¼ï¼šCSV æ–‡ä»¶ï¼ŒåŒ…å« 'dirty', 'clean', 'confidence' åˆ—

ä½¿ç”¨ç¤ºä¾‹ï¼š
  # åŸºæœ¬æ‰¹é‡æ¨ç†
  python src/batch_inference.py --ckpt ckpts/norm-byt5-cpu/best --input test_data/input_samples.csv --output results/inference_results.csv
  
  # æŒ‡å®šé•œåƒå’Œç¼“å­˜ï¼ˆå›½å†…ç¯å¢ƒæ¨èï¼‰
  python src/batch_inference.py --ckpt ckpts/norm-byt5-cpu/best --input test_data/input_samples.csv --output results/inference_results.csv --hf-endpoint https://hf-mirror.com --hf-home ./.hf_cache
  
  # ç¦»çº¿æ¨¡å¼
  python src/batch_inference.py --ckpt ./local_models/byt5-small --input test_data/input_samples.csv --output results/inference_results.csv --offline

é€‚ç”¨ç¯å¢ƒï¼šWindows + Conda + å›½å†…ç½‘ç»œç¯å¢ƒ
ä½œè€…ï¼šMaterial-Code-Manager é¡¹ç›®ç»„
"""

import os
import re as pyre
import time
import argparse
import threading
import pandas as pd
import numpy as np
import torch
import json
import os.path
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

RUNNING = True

def log(msg: str):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def heartbeat(msg="ğŸ’¡ æ‰¹é‡æ¨ç†ä¸­..."):
    while RUNNING:
        time.sleep(10)
        print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--ckpt", type=str, required=True, help="æ¨¡å‹/æƒé‡ç›®å½•æˆ–æ¨¡å‹å")
    ap.add_argument("--input", type=str, required=True, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„")
    ap.add_argument("--output", type=str, required=True, help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    ap.add_argument("--input_column", type=str, default="dirty", help="è¾“å…¥åˆ—åï¼ˆé»˜è®¤ï¼šdirtyï¼‰")
    ap.add_argument("--batch_size", type=int, default=1, help="æ‰¹å¤„ç†å¤§å°")
    ap.add_argument("--regex", type=str,
                    default=r"^[\p{Han}A-Za-z0-9\-\_/ï¼\.,ï¼Œï¼ˆï¼‰\(\)=+\*Ã—ï¼/:ï¼š;ï¼›\[\]ã€ã€‘<>ã€Šã€‹\s]{1,120}$",
                    help="ç”Ÿæˆååˆæ³•æ€§æ ¡éªŒç”¨çš„å®½æ¾æ­£åˆ™")
    ap.add_argument("--beams", type=int, default=1, help="æŸæœç´¢å¤§å°")
    ap.add_argument("--max_new_tokens", type=int, default=32, help="ç”Ÿæˆæœ€å¤§æ–°tokenæ•°")
    # è¿è¡Œç¯å¢ƒç›¸å…³
    ap.add_argument("--offline", action="store_true", help="å®Œå…¨ç¦»çº¿")
    ap.add_argument("--hf-endpoint", type=str, default="https://hf-mirror.com", help="Hugging Face é•œåƒç«¯ç‚¹")
    ap.add_argument("--hf-home", type=str, default=".\.hf_cache", help="HF ç¼“å­˜ç›®å½•")
    return ap.parse_args()

def calculate_confidence(logits, generated_ids, tokenizer):
    """è®¡ç®—ç”Ÿæˆç»“æœçš„ç½®ä¿¡åº¦"""
    try:
        # è·å–ç”Ÿæˆåºåˆ—çš„æ¦‚ç‡
        probs = torch.softmax(logits, dim=-1)
        # è®¡ç®—ç”Ÿæˆtokençš„å¹³å‡æ¦‚ç‡ä½œä¸ºç½®ä¿¡åº¦
        token_probs = []
        for i, token_id in enumerate(generated_ids[0]):
            if i < len(logits[0]):
                token_prob = probs[0][i][token_id].item()
                token_probs.append(token_prob)
        
        if token_probs:
            confidence = np.mean(token_probs)
        else:
            confidence = 0.0
        
        return confidence
    except Exception:
        return 0.0

def main():
    global RUNNING
    
    args = parse_args()
    
    # 1) è®¾ç½®ç¯å¢ƒ
    log("==== ğŸ§­ è®¾ç½®è¿è¡Œç¯å¢ƒ ====")
    # è®¾ç½®é»˜è®¤æˆ–ç”¨æˆ·æŒ‡å®šçš„ HF é…ç½®
    os.environ["HF_ENDPOINT"] = args.hf_endpoint
    os.environ["HF_HOME"] = args.hf_home
    if args.offline:
        os.environ["TRANSFORMERS_OFFLINE"] = "1"

    log(f"HF_ENDPOINT={os.environ.get('HF_ENDPOINT')}")
    log(f"HF_HOME={os.environ.get('HF_HOME')}")
    log(f"TRANSFORMERS_OFFLINE={os.environ.get('TRANSFORMERS_OFFLINE')}")
    
    print("\n==== ğŸš€ æ‰¹é‡æ¨ç†å¯åŠ¨ ====\n", flush=True)

    # å¿ƒè·³çº¿ç¨‹
    t = threading.Thread(target=heartbeat, daemon=True)
    t.start()

    # 2) è¯»å–è¾“å…¥æ•°æ®
    log("==== ğŸ“– è¯»å–è¾“å…¥æ•°æ® ====")
    if not os.path.exists(args.input):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
    
    df = pd.read_csv(args.input)
    log(f"è¯»å–åˆ° {len(df)} æ¡æ•°æ®")
    
    if args.input_column not in df.columns:
        raise ValueError(f"è¾“å…¥åˆ— '{args.input_column}' ä¸å­˜åœ¨äºCSVæ–‡ä»¶ä¸­")

    # 3) åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
    log("==== ğŸ¤– åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ====")
    local_only = os.environ.get("TRANSFORMERS_OFFLINE") == "1"
    name_or_path = args.ckpt
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA é€‚é…å™¨æ¨¡å‹
    adapter_config_path = os.path.join(name_or_path, "adapter_config.json")
    if os.path.exists(adapter_config_path):
        log("ğŸ”§ æ£€æµ‹åˆ° LoRA é€‚é…å™¨ï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹å’Œé€‚é…å™¨")
        # è¯»å–é€‚é…å™¨é…ç½®è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
        with open(adapter_config_path, 'r', encoding='utf-8') as f:
            adapter_config = json.load(f)
        base_model_name = adapter_config.get('base_model_name_or_path', 'google/byt5-small')
        
        try:
            # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
            tok = AutoTokenizer.from_pretrained(base_model_name, local_files_only=local_only)
            mdl = AutoModelForSeq2SeqLM.from_pretrained(base_model_name, local_files_only=local_only)
            # ç„¶ååŠ è½½ LoRA é€‚é…å™¨
            mdl = PeftModel.from_pretrained(mdl, name_or_path, local_files_only=local_only)
            log("âœ… LoRA æ¨¡å‹åŠ è½½æˆåŠŸ")
        except OSError as e:
            log(f"âš ï¸ åœ¨çº¿/ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°è¯•æœ¬åœ°åŠ è½½ï¼š{e}")
            tok = AutoTokenizer.from_pretrained(base_model_name, local_files_only=True)
            mdl = AutoModelForSeq2SeqLM.from_pretrained(base_model_name, local_files_only=True)
            mdl = PeftModel.from_pretrained(mdl, name_or_path, local_files_only=True)
            log("âœ… LoRA æœ¬åœ°åŠ è½½æˆåŠŸ")
    else:
        # æ™®é€šæ¨¡å‹åŠ è½½
        try:
            tok = AutoTokenizer.from_pretrained(name_or_path, local_files_only=local_only)
            mdl = AutoModelForSeq2SeqLM.from_pretrained(name_or_path, local_files_only=local_only)
        except OSError as e:
            log(f"âš ï¸ åœ¨çº¿/ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°è¯•æœ¬åœ°åŠ è½½ï¼š{e}")
            tok = AutoTokenizer.from_pretrained(name_or_path, local_files_only=True)
            mdl = AutoModelForSeq2SeqLM.from_pretrained(name_or_path, local_files_only=True)
            log("âœ… æœ¬åœ°åŠ è½½æˆåŠŸ")

    mdl.eval()

    # 4) æ‰¹é‡æ¨ç†
    log("==== âœ¨ å¼€å§‹æ‰¹é‡æ¨ç† ====")
    results = []
    
    # å‡†å¤‡æ­£åˆ™è¡¨è¾¾å¼
    try:
        import regex as re
        rgx = re.compile(args.regex)
        use_regex = True
    except Exception:
        use_regex = False
        log("âš ï¸ regexåº“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å†…ç½®reåº“è¿›è¡Œç®€å•æ¸…ç†")

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="æ¨ç†è¿›åº¦"):
        try:
            input_text = str(row[args.input_column])
            prefix = "normalize: " + input_text.strip()
            inputs = tok([prefix], return_tensors="pt")
            
            with torch.no_grad():
                # ç”Ÿæˆï¼ŒåŒæ—¶è¿”å›scoresç”¨äºè®¡ç®—ç½®ä¿¡åº¦
                output = mdl.generate(
                    **inputs,
                    max_new_tokens=args.max_new_tokens,
                    num_beams=args.beams,
                    return_dict_in_generate=True,
                    output_scores=True,
                    do_sample=False
                )
                
                generated_ids = output.sequences
                scores = output.scores if hasattr(output, 'scores') else None
            
            # è§£ç ç»“æœ
            pred = tok.decode(generated_ids[0], skip_special_tokens=True).replace("normalize:", "").strip()
            
            # è®¡ç®—ç½®ä¿¡åº¦
            if scores:
                confidence = calculate_confidence(scores, generated_ids, tok)
            else:
                confidence = 0.0
            
            # æ­£åˆ™å…œåº•æ¸…ç†
            if use_regex:
                if not rgx.match(pred):
                    pred = re.sub(r"[\p{C}]", "", pred).strip()
            else:
                pred = pyre.sub(r"[\x00-\x1F\x7F]", "", pred).strip()
            
            # ä¿å­˜ç»“æœ
            result = {
                'input': input_text,
                'output': pred,
                'confidence': round(confidence, 4)
            }
            # ä¿ç•™åŸå§‹æ•°æ®çš„å…¶ä»–åˆ—
            for col in df.columns:
                if col != args.input_column:
                    result[col] = row[col]
            
            results.append(result)
            
        except Exception as e:
            log(f"âŒ å¤„ç†ç¬¬ {idx+1} è¡Œæ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶ä¹Ÿä¿å­˜ä¸€æ¡è®°å½•
            result = {
                'input': str(row[args.input_column]) if args.input_column in row else '',
                'output': f"ERROR: {str(e)}",
                'confidence': 0.0
            }
            for col in df.columns:
                if col != args.input_column:
                    result[col] = row[col]
            results.append(result)

    # 5) ä¿å­˜ç»“æœ
    log("==== ğŸ’¾ ä¿å­˜ç»“æœ ====")
    results_df = pd.DataFrame(results)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    results_df.to_csv(args.output, index=False, encoding='utf-8')
    log(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    log(f"å…±å¤„ç† {len(results)} æ¡æ•°æ®")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    avg_confidence = results_df['confidence'].mean()
    log(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.4f}")
    
    RUNNING = False
    print("\n==== âœ… æ‰¹é‡æ¨ç†å®Œæˆ ====\n", flush=True)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        RUNNING = False
        log("ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰")
    except Exception as ex:
        RUNNING = False
        log(f"âŒ ç¨‹åºå¼‚å¸¸ï¼š{ex}")
        raise
